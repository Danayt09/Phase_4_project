{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b4f97e7-799e-41d8-aa17-b689f6384fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danayt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/danayt/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /Users/danayt/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/danayt/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/danayt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag, bigrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "import nltk.collocations as collocations\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,  confusion_matrix, ConfusionMatrixDisplay,\\\n",
    "precision_recall_fscore_support, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, KFold, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0a9f444-cef5-4061-89cd-99f7152ffb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../raw_data/judge-1377884607_tweet_product_company.csv\", encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30575c98-20a6-4ed8-9800-c4e859e9a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dopping null column from tweet_text column since it's only one and has no information for our analysis\n",
    "data.tweet_text.isna().sum()\n",
    "data.dropna(subset=['tweet_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d3d46e0-edf2-4422-b438-de942f605043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at our .describe, there seems to be some duplicates in our data and we'll go ahead and keep unique inputs only\n",
    "data.drop_duplicates(subset=['tweet_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "101c9868-e2fc-42b9-8897-31986d3d23ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a cleaned emotion_in tweet_is_directed_at column\n",
    "data.rename(columns={'emotion_in_tweet_is_directed_at': 'brand'}, inplace=True)\n",
    "data.rename(columns={'is_there_an_emotion_directed_at_a_brand_or_product': 'sentiment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df195acb-d5cf-4877-bd97-9ddc6e94f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"Apple\", \"Google\", \"iPad\", \"iPhone\", \"Android\"]\n",
    "\n",
    "for keyword in keywords:\n",
    "    data[keyword] = data['tweet_text'].str.extract(f'({keyword})', flags=re.IGNORECASE)\n",
    "\n",
    "data['product'] = data[['Apple', 'iPhone', 'iPad', 'Google', 'Android']].apply(lambda x: 'Apple' if x[['Apple', 'iPhone', 'iPad']].any() \n",
    "                                                                                else 'Google' if x[['Google', 'Android']].any() \n",
    "                                                                                else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e011b152-32a6-403b-b859-a25093a75857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product\n",
       "Apple     5517\n",
       "Google    2764\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ec84b41-bd2a-4b65-b2fa-481300853d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we've classified our product column, we'll drop nulls as we're interested with apple and google products\n",
    "data['product'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b727ec86-d94c-4d86-83a3-b929d6e5f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['product'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6255b78-aaa1-4b06-99c3-1c9040015d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'].replace(\"I can't tell\" , \"Neutral emotion\", inplace=True)\n",
    "data['sentiment'].replace(\"No emotion toward brand or product\" , \"Neutral emotion\", inplace=True)\n",
    "data['sentiment'].replace(\"No emotion toward brand or product\" , \"Neutral emotion\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a70de33-516c-4377-8976-5b35d3ed1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_to_replace = {'Positive emotion':2, 'Neutral emotion':1,\n",
    "                   'Negative emotion':0}\n",
    "\n",
    "data['sentiment']  = data['sentiment'].replace(target_to_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d29e45f-5e02-4c8d-9351-27be5c27d46f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    4783\n",
       "2    2930\n",
       "0     568\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef8092a6-b443-490c-b582-da938455926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "sw.extend([\"apple\", \"google\", \"ipad\", \"iphone\", \"android\", \"sxsw\", \"link\", \"mention\", \"sxwsi\", \"rt\", \"hmmm\", \"sxswu\", \"goog\", \"iusxsw\",\n",
    "           \"etc\", \"via\", \"today\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec9659cd-9925-44fe-b72c-08384f84ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tokenizer\n",
    "def doc_preparer(doc, stop_words=sw):\n",
    "    '''\n",
    "    \n",
    "    :param doc: a document from the satire corpus \n",
    "    :return: a document string with words which have been \n",
    "            lemmatized, \n",
    "            parsed for stopwords, \n",
    "            made lowercase,\n",
    "            and stripped of punctuation and numbers.\n",
    "    '''\n",
    "    doc = unicodedata.normalize('NFKD', doc).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:â€™[a-z]+)?)\")\n",
    "    doc = regex_token.tokenize(doc)\n",
    "    doc = [word.lower() for word in doc]\n",
    "    doc = [word for word in doc if word not in sw]\n",
    "    # print(doc)\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(word[0], get_wordnet_pos(word[1])) for word in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    return ' '.join(doc)\n",
    " # Translate nltk POS to wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3178302-0e83-4a93-a114-492c7f29de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting important features from our data \n",
    "corpse = data.tweet_text\n",
    "target = data.sentiment\n",
    "product = data.product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7c1617a-bf5f-4c4a-874c-6142b0459bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [doc_preparer(doc, sw) for doc in corpse]\n",
    "y = data.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4265a9e-34c7-4f16-b29c-05d149cd155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64f5a659-af99-428b-9372-876f1a830af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train,\n",
    "                                          test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5977c09-859f-4501-91a3-65e55cf79b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ui\n",
    "#lt\n",
    "#rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df188393-1623-4a7a-9760-2cbbf81fb716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada4e121-9b61-4e05-b00c-6054c8ade87c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71882632-13cc-4d9c-afbc-6f3e04f7b08f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def5090-29a9-45e9-aac8-69f67083b1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db91859-9433-4b41-bc43-9784091a77e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
