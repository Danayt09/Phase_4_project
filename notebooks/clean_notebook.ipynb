{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b4f97e7-799e-41d8-aa17-b689f6384fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danayt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/danayt/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to /Users/danayt/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/danayt/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /Users/danayt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import pos_tag, bigrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "import nltk.collocations as collocations\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,  confusion_matrix, ConfusionMatrixDisplay,\\\n",
    "precision_recall_fscore_support, f1_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, KFold, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a9f444-cef5-4061-89cd-99f7152ffb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../raw_data/judge-1377884607_tweet_product_company.csv\", encoding='unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30575c98-20a6-4ed8-9800-c4e859e9a15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dopping null column from tweet_text column since it's only one and has no information for our analysis\n",
    "data.tweet_text.isna().sum()\n",
    "data.dropna(subset=['tweet_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d3d46e0-edf2-4422-b438-de942f605043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at our .describe, there seems to be some duplicates in our data and we'll go ahead and keep unique inputs only\n",
    "data.drop_duplicates(subset=['tweet_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "101c9868-e2fc-42b9-8897-31986d3d23ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a cleaned emotion_in tweet_is_directed_at column\n",
    "data.rename(columns={'emotion_in_tweet_is_directed_at': 'brand'}, inplace=True)\n",
    "data.rename(columns={'is_there_an_emotion_directed_at_a_brand_or_product': 'sentiment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df195acb-d5cf-4877-bd97-9ddc6e94f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"Apple\", \"Google\", \"iPad\", \"iPhone\", \"Android\", \"goog\"]\n",
    "\n",
    "for keyword in keywords:\n",
    "    data[keyword] = data['tweet_text'].str.extract(f'({keyword})', flags=re.IGNORECASE)\n",
    "\n",
    "data['product'] = data[['Apple', 'iPhone', 'iPad', 'Google', 'Android']].apply(lambda x: 'Apple' if x[['Apple', 'iPhone', 'iPad']].any() \n",
    "                                                                                else 'Google' if x[['Google', 'Android']].any() \n",
    "                                                                                else None, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79954635-1e73-4795-bd46-febe77ade811",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>brand</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Apple</th>\n",
       "      <th>Google</th>\n",
       "      <th>iPad</th>\n",
       "      <th>iPhone</th>\n",
       "      <th>Android</th>\n",
       "      <th>product</th>\n",
       "      <th>text_with_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>Ipad everywhere. #SXSW {link}</td>\n",
       "      <td>iPad</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ipad</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apple</td>\n",
       "      <td>everywhere Apple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>Wave, buzz... RT @mention We interrupt your re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>google</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google</td>\n",
       "      <td>wave buzz interrupt regularly schedule geek pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>Google's Zeiger, a physician never reported po...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google</td>\n",
       "      <td>zeiger physician never report potential ae yet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>Some Verizon iPhone customers complained their...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apple</td>\n",
       "      <td>verizon customer complain time fell back hour ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Google</td>\n",
       "      <td>iiau eioaaa uart test uicheck offersu Google</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text brand  sentiment  \\\n",
       "9088                      Ipad everywhere. #SXSW {link}  iPad          2   \n",
       "9089  Wave, buzz... RT @mention We interrupt your re...   NaN          1   \n",
       "9090  Google's Zeiger, a physician never reported po...   NaN          1   \n",
       "9091  Some Verizon iPhone customers complained their...   NaN          1   \n",
       "9092  Ï¡Ïàü_ÊÎÒ£Áââ_£â_ÛâRT @...   NaN          1   \n",
       "\n",
       "     Apple  Google  iPad  iPhone Android product  \\\n",
       "9088   NaN     NaN  Ipad     NaN     NaN   Apple   \n",
       "9089   NaN  google   NaN     NaN     NaN  Google   \n",
       "9090   NaN  Google   NaN     NaN     NaN  Google   \n",
       "9091   NaN     NaN   NaN  iPhone     NaN   Apple   \n",
       "9092   NaN  Google   NaN     NaN     NaN  Google   \n",
       "\n",
       "                                      text_with_product  \n",
       "9088                                   everywhere Apple  \n",
       "9089  wave buzz interrupt regularly schedule geek pr...  \n",
       "9090  zeiger physician never report potential ae yet...  \n",
       "9091  verizon customer complain time fell back hour ...  \n",
       "9092       iiau eioaaa uart test uicheck offersu Google  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e011b152-32a6-403b-b859-a25093a75857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product\n",
       "Apple     5517\n",
       "Google    2764\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ec84b41-bd2a-4b65-b2fa-481300853d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we've classified our product column, we'll drop nulls as we're interested with apple and google products\n",
    "data['product'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b727ec86-d94c-4d86-83a3-b929d6e5f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=['product'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6255b78-aaa1-4b06-99c3-1c9040015d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sentiment'].replace(\"I can't tell\" , \"Neutral emotion\", inplace=True)\n",
    "data['sentiment'].replace(\"No emotion toward brand or product\" , \"Neutral emotion\", inplace=True)\n",
    "data['sentiment'].replace(\"No emotion toward brand or product\" , \"Neutral emotion\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a70de33-516c-4377-8976-5b35d3ed1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_to_replace = {'Positive emotion':2, 'Neutral emotion':1,\n",
    "                   'Negative emotion':0}\n",
    "\n",
    "data['sentiment']  = data['sentiment'].replace(target_to_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d29e45f-5e02-4c8d-9351-27be5c27d46f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "1    4783\n",
       "2    2930\n",
       "0     568\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e7a2948-0657-4b3e-9d9f-f4ae9a9d8274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product\n",
       "Apple     5517\n",
       "Google    2764\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['product'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cb4f8ab-173d-4712-968d-e0df51e57a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['product'] = data['product'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef8092a6-b443-490c-b582-da938455926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "sw.extend([\"apple\", \"google\", \"ipad\", \"iphone\", \"android\", \"sxsw\", \"link\", \"mention\", \"sxwsi\", \"hmmm\", \"sxswu\", \"goog\", \"iusxsw\",\n",
    "           \"etc\", \"via\", \"today\", \"hmmmm\", \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec9659cd-9925-44fe-b72c-08384f84ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tokenizer\n",
    "def doc_preparer(doc, stop_words=sw):\n",
    "\n",
    "    #lemmatized, parsed for stopwords, made lowercase,and stripped of punctuation and numbers.\n",
    "\n",
    "    doc = unicodedata.normalize('NFKD', doc).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    # Remove URLs\n",
    "    doc = re.sub(r'http\\S+|www\\S+|https\\S+', '', doc, flags=re.MULTILINE)\n",
    "    # Remove mentions and hashtags\n",
    "    doc = re.sub(r'\\@\\w+|\\#','', doc)\n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:’[a-z]+)?)\")\n",
    "    doc = regex_token.tokenize(doc)\n",
    "    doc = [word.lower() for word in doc]\n",
    "    doc = [word for word in doc if word not in sw]\n",
    "    # print(doc)\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(word[0], get_wordnet_pos(word[1])) for word in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    return ' '.join(doc)\n",
    " # Translate nltk POS to wordnet tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3178302-0e83-4a93-a114-492c7f29de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting important features from our data \n",
    "corpse = data.tweet_text\n",
    "target = data.sentiment\n",
    "product = data.product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87846916-20f5-40cf-a360-77b8421cfd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_text = [doc_preparer(doc,sw) for doc in corpse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f94805-9871-4473-92db-6938c7800b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if word is greater than 1\n",
    "# tokens = [word for word in tokens if len(word) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5d80acc-6236-4db9-bd58-d269e40ef9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'product' column to strings\n",
    "data['product'] = data['product'].astype(str)\n",
    "\n",
    "# Concatenate the \"corpse\" and \"product\" columns\n",
    "data['text_with_product'] = [text + ' ' + product for text, product in zip(token_text, data['product'])]\n",
    "\n",
    "# Assign the concatenated text to X\n",
    "X = data['text_with_product']\n",
    "\n",
    "# Assign the target labels to y\n",
    "y = data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4265a9e-34c7-4f16-b29c-05d149cd155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64f5a659-af99-428b-9372-876f1a830af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, X_val, y_t, y_val = train_test_split(X_train, y_train,\n",
    "                                          test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5977c09-859f-4501-91a3-65e55cf79b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ui\n",
    "#lt\n",
    "#rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10a6d224-13e5-4aef-ad54-ebff41e02880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65306777454094"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X_t_cv = cv.fit_transform(X_t)\n",
    "\n",
    "X_val_cv = cv.transform(X_val)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_cv, y_t)\n",
    "y_hat = mnb.predict(X_val_cv)\n",
    "\n",
    "precision_score(y_val, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def5090-29a9-45e9-aac8-69f67083b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = mnb.predict(X_val)\n",
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1db91859-9433-4b41-bc43-9784091a77e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6848074055439056"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_t_vec = tfidf.fit_transform(X_t)\n",
    "\n",
    "X_val_vec = tfidf.transform(X_val)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "\n",
    "precision_score(y_val, y_hat, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "613f6132-6ef0-444d-b219-99feb10a7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to pull a specific predictor out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da76dcc8-b4a5-4899-8651-8f70420338b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
